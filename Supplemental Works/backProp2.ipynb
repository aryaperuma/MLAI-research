{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lec 54 and 55.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP5nk1ZWb8cwjDOjz2t7pBY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"l1SdPSANedSY"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ru9xsB7bWllk"},"source":["Functions"]},{"cell_type":"code","metadata":{"id":"8n-T2pBD0KOL"},"source":["def sigmoid(x):\n","  return 1/(1+np.exp(-x))\n","\n","def relu(x):\n","  return np.maximum(0,x)  \n","\n","def softmax(X):\n","  return np.exp(X)/(np.sum(np.exp(X),axis=1).reshape(-1,1))   \n","\n","def sigmoid_p(x):\n","  return sigmoid(x)*(1-sigmoid(x))\n","\n","def relu_p(x):\n","  return 1*(x >= 0)    \n","  \n","def h(act,x):\n","  if act == 'sigmoid':\n","    return sigmoid(x)\n","  if act == 'relu':\n","    return relu(x)\n","  if act == 'identity':\n","    return x\n","  if act == 'tanh':\n","    return np.tanh(x)\n","  if act == 'softmax':\n","    return softmax(x)\n","  return 'Problem'  \n","\n","def h_p(act,x):\n","  if act == 'sigmoid':\n","    return sigmoid_p(x)\n","  if act == 'relu':\n","    return relu_p(x)\n","  if act == 'identity':\n","    return np.ones(x.shape)\n","  if act == 'tanh':\n","    return 1/(np.cosh(x))**2\n","  return 'Problem'    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"21u1KbxlWhH0"},"source":["One fully connected layer"]},{"cell_type":"code","metadata":{"id":"YOs1Tg22zlY6"},"source":["def affine(b,W,X):\n","  return b + np.matmul(X,W)\n","\n","# When a layer is fully connected\n","# input: b, W, parameters of the layer, activ is a string that tell us \n","# the activation function of the layer, X data of the previous layer (after \n","# flatteting if necessary, so X is a 2d tensor)\n","# output: A, data of the layer, and the corresponding Z\n","def fully_connected(b,W,activ,X):\n","  Z = affine(b,W,X)  \n","  A = h(activ,Z)\n","  return A, Z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFsfMwlUWpF8"},"source":["One convolutional layer"]},{"cell_type":"code","metadata":{"id":"IPI5fqflwmDB"},"source":["def padding(p,A):\n","  return np.pad(A,((0,0),(p,p),(p,p),(0,0)))\n","\n","def conv(W,s,A):\n","  f = W.shape[0]\n","  n_e = A.shape[0]\n","  n_h = (A.shape[1]-f)//s + 1\n","  n_w = (A.shape[2]-f)//s + 1\n","  n_c = W.shape[3]\n","  A_conv_W = np.zeros((n_e,n_h,n_w,n_c))\n","  for i_e in range(n_e):\n","    for i_h in range(n_h):\n","      for i_w in range(n_w):\n","        for i_c in range(n_c):\n","          A_conv_W[i_e,i_h,i_w,i_c] = np.sum(A[i_e,i_h*s:i_h*s+f,i_w*s:i_w*s+f,:]*W[:,:,:,i_c])\n","  return A_conv_W  \n","\n","# When a layer is convolutional \n","# input: b, W, parameters of the layer, s =stride, p = padding, activ is a \n","# string that tell us the activation function of the layer, X data of the previous layer \n","# (X is a 4d tensor)\n","# output: A, data of the layer, and the corresponding Z\n","\n","def convolutional(b,W,s,p,activ,X):\n","  Z = conv(W,s,padding(p,X))+b\n","  A = h(activ,Z)\n","  return A, Z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZadmIyJWtJM"},"source":["One pooling layer"]},{"cell_type":"code","metadata":{"id":"y3e_i0lgjvQb"},"source":["def pooling(f,s,A):\n","  n_e = A.shape[0]\n","  n_h = (A.shape[1]-f)//s + 1\n","  n_w = (A.shape[2]-f)//s + 1\n","  n_c = A.shape[3]\n","  A_pool = np.zeros((n_e,n_h,n_w,n_c))\n","  Z = []\n","  for i_e in range(n_e):\n","    for i_h in range(n_h):\n","      for i_w in range(n_w):\n","        for i_c in range(n_c):\n","          x = np.argmax(A[i_e,i_h*s:i_h*s+f,i_w*s:i_w*s+f,i_c])\n","          i_w_max = x%f + i_w*s\n","          i_h_max = x//f + i_h*s\n","          Z.append((i_e,i_h_max,i_w_max,i_c))\n","          A_pool[i_e,i_w,i_h,i_c] = A[i_e,i_w_max,i_h_max,i_c]\n","  return A_pool, Z  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1q0naT-F-IJq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vcfk-BoF5q8s"},"source":["# Input data\n","#\n","# Features in X. X.shape = (n_e, n_h, n_w, n_c) n_e = number of examples\n","# n_c = 3 if color images. n_c = 1 if black and white image \n","#\n","# Labels in Y. Y.shape = (n_e, n_cat) n_cat = number of categories\n","\n","# Type network (hyperparameters)\n","#\n","# activation = [None, '','',....... ] relu, sigmoid, tanh, softmax, lin\n","# layer_type = [None, '','',.....] convolutional_layer, pooling_layer, dense_layer\n","# shape_data = [X.shape[1:], (), (),....] shape of the data in the layer without \n","# the first component, that corresponds to the number of example\n","# paddings = [None,,,.....] some are numbers, some are None\n","# strides = [None,,,.....] some are numbers, some are None\n","# filters_sizes = [None,,,.....] some are numbers, some are None\n","# L = number of layers not counting the input layer \n","# L = len(shape_data) - 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mi_tC6LGW0Nl"},"source":["Finding the first fully connected layer"]},{"cell_type":"code","metadata":{"id":"qXx8leqIW5Dk"},"source":["def first_dense_layer(layer_type):\n","  l = 1\n","  while layer_type[l] != 'dense_layer':\n","    l = l+1 \n","  return l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5aeHbs7cW731"},"source":["Computing the A_l, data in the layers, and Z_l, the data before applying activation or the indices in the pooling layers "]},{"cell_type":"code","metadata":{"id":"Krohm3MM3ApN"},"source":["def data_next_layer(W,b,activ,layer_type,filter_size,stride,p,A):\n","  if layer_type == 'dense_layer':\n","    return fully_connected(b,W,activ,A)\n","  if layer_type == 'convolutional_layer':\n","    return convolutional(b,W,stride,p,activ,A)\n","  if layer_type == 'pooling_layer':\n","    return pooling(filter_size,stride,A)\n","\n","def data_in_layers(W,b,activation,layer_type,filters_sizes,strides,paddings,first_dense,L,X):\n","  A = [X]\n","  Z = [None]\n","  for l in range(1,L+1):\n","    A_prev = A[l-1]\n","    if l == first_dense:\n","      A_prev = A_prev.reshape((A_prev.shape[0],-1))\n","    A_l, Z_l = data_next_layer(W[l],b[l],activation[l],layer_type[l],filters_sizes[l],strides[l],paddings[l],A_prev)\n","    A.append(A_l)\n","    Z.append(Z_l)\n","  return A, Z  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bj5jQGe8XmLf"},"source":["Predictions"]},{"cell_type":"code","metadata":{"id":"uusC0SUsXiW_"},"source":["def predictions(W,b,activation,layer_type,filters_sizes,strides,paddings,first_dense,L,X):\n","  A, Z = data_in_layers(W,b,activation,layer_type,filters_sizes,strides,paddings,first_dense,L,X)\n","  return A[-1]    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwD7ohz6YvIa"},"source":["Cross entropy error"]},{"cell_type":"code","metadata":{"id":"3ZgLqYGy7qrq"},"source":["def error_cross_entropy(Y,Y_hat):\n","  return -np.sum(Y*np.log(Y_hat))/Y.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ndLIu1yrbDi2"},"source":["DJ/DA in the last layer"]},{"cell_type":"code","metadata":{"id":"BjtiU1K6Y_Oo"},"source":["def DJ_DA_L(Y,A_L):\n","  return -(Y/A_L)/Y.shape[0]  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TX9gp4tqUre5"},"source":["DJ/DZ in the last layer"]},{"cell_type":"code","metadata":{"id":"FF2gQ2XgUs73","executionInfo":{"status":"ok","timestamp":1617815603609,"user_tz":240,"elapsed":448,"user":{"displayName":"ggoldsztein@yahoo.com","photoUrl":"","userId":"06354275666408843048"}}},"source":["def DJ_DZ_L(Y,A_L):\n","  return (A_L-Y)/Y.shape[0]  "],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SmSPIabAbJVy"},"source":["One fully connected layer. Backward to compute gradients"]},{"cell_type":"code","metadata":{"id":"gilvUF12GrrU"},"source":["def gradients_one_layer_dense(l,activ,b,W,Z,A,A_prev,DJ_DA,Y,first_dense):\n","  if activ == 'softmax':\n","    DJ_DZ = DJ_DZ_L(Y,A)\n","  else:  \n","    DJ_DZ = DJ_DA*h_p(activ,Z)  \n","  DJ_Db = np.sum(DJ_DZ,axis=0)\n","  if l == first_dense:\n","    A_prev = A_prev.reshape((A_prev.shape[0],-1))\n","  DJ_DW = np.matmul(A_prev.T,DJ_DZ)\n","  DJ_DA_prev = None\n","  if l != 1:\n","    DJ_DA_prev = np.matmul(DJ_DZ,W.T)\n","  return DJ_Db, DJ_DW, DJ_DA_prev"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MrcWZ1wwby5s"},"source":["One convolutional layer. Backward to compute gradients"]},{"cell_type":"code","metadata":{"id":"ebXScE_JHHUt"},"source":["def DJ_DW_conv(W_shape,DJ_DZ,A_prev_pad,s) \n","  DJ_DW = np.zeros(W_shape)\n","  for i1 in range(W_shape[0]):\n","    ind_1 = np.arange(i1,(DJ_DZ.shape[1]-1)*s+i1+1,s)\n","    for i2 in range(W_shape[1]):\n","      ind_2 = np.arange(i2,(DJ_DZ.shape[2]-1)*s+i2+1,s)\n","      for i3 in range(W_shape[2]):\n","        A_res = A_prev_pad[:,:,:,i3]\n","        A_res = A_res[:,:,ind_2]\n","        A_res = A_res[:,ind_1,:]\n","        for i4 in range(W_shape[3]):\n","          DJ_DW[i1,i2,i3,i4] = np.sum(DJ_DZ[:,:,:,i4]*A_res)\n","  return DJ_DW    \n","\n","def DJ_DA_prev_conv(A_prev_shape,DJ_DZ,W,p,s):\n","  f = W.shape[0]\n","  DJ_DA_prev = np.zeros(A_prev_shape)\n","  for i1 in range(A_prev_shape[0]):\n","    for i2 in range(A_prev_shape[1]):\n","      a2_min = max((i2+p-f)//s+1,0)\n","      a2_max = min((i2+p)//s,DJ_DZ.shape[1]-1)\n","      ind_0 = np.arange(i2+p-a2_min*s,i2+p-a2_max*s-1,-s)\n","      for i3 in range(A_prev_shape[2]):\n","        a3_min = max((i3+p-f)//s+1,0)\n","        a3_max = min((i3+p)//s,DJ_DZ.shape[2]-1)\n","        ind_1 = np.arange(i3+p-a3_min*s,i3+p-a3_max*s-1,-s)\n","        DJ_DZ_res = DJ_DZ[i1,:,:,:]\n","        DJ_DZ_res = DJ_DZ_res[:,a3_min:a3_max+1,:]\n","        DJ_DZ_res = DJ_DZ_res[a2_min:a2_max+1,:,:]\n","        for i4 in range(A_prev_shape[3]):\n","          W_res = W[:,:,i4,:]\n","          W_res = W_res[:,ind_1,:]\n","          W_res = W_res[ind_0,:,:]\n","          DJ_DA_prev[i1,i2,i3,i4] = np.sum(DJ_DZ_res*W_res)\n","  return DJ_DA_prev\n","\n","def gradients_one_layer_convolutional(l,activ,p,s,b,W,Z,A,A_prev,DJ_DA):\n","  A_prev_pad = padding(p,A_prev)\n","  DJ_DZ = DJ_DA*h_p(activ,Z)\n","  DJ_Db = np.sum(DJ_DZ,axis=(0,1,2))\n","  DJ_DW = DJ_DW_conv(W.shape,DJ_DZ,A_prev_pad,s)\n","  DJ_DA_prev = None\n","  if l != 1:\n","    DJ_DA_prev = DJ_DA_prev_conv(A_prev.shape,DJ_DZ,W,p,s)\n","  return DJ_Db, DJ_DW, DJ_DA_prev"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vAA6uHmLb1i9"},"source":["One pooling layer. Backward to compute gradients"]},{"cell_type":"code","metadata":{"id":"BeiOGYm3KXNu"},"source":["def gradients_one_layer_pooling(l,Z,A,A_prev,DJ_DA):\n","  n_e = A.shape[0]\n","  n_h = A.shape[1]\n","  n_w = A.shape[2]\n","  n_c = A.shape[3]\n","  DJ_DA_prev = np.zeros(A_prev.shape)\n","  if l == 1:\n","    return None, None, DJ_DA_prev\n","  i = 0\n","  for i_e in range(n_e):\n","    for i_h in range(n_h):\n","      for i_w in range(n_w):\n","        for i_c in range(n_c):\n","          DJ_DA_prev[Z[i]] = DJ_DA_prev[Z[i]] + DJ_DA[i_e,i_h,i_w,i_c] \n","          i = i+1 \n","  return None, None, DJ_DA_prev"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IklukF_s-Po5"},"source":["Gradients in all the layers"]},{"cell_type":"code","metadata":{"id":"nuqTzkJXIvoX"},"source":["def gradients_one_layer(l,layer_type,activ,p,s,b,W,Z,A,A_prev,DJ_DA,Y,first_dense):\n","  if layer_type == 'dense_layer':\n","    return gradients_one_layer_dense(l,activ,b,W,Z,A,A_prev,DJ_DA,Y,first_dense)\n","  if layer_type == 'convolutional_layer':\n","    return gradients_one_layer_convolutional(l,activ,p,s,b,W,Z,A,A_prev,DJ_DA)\n","  if layer_type == 'pooling_layer':\n","    return gradients_one_layer_pooling(l,Z,A,A_prev,DJ_DA)\n","\n","def gradients_all_layers(W,b,activation,layer_type,strides,paddings,first_dense,L,A,Z,Y):\n","  DJ_DA = [DJ_DA_L(Y,A[-1])]\n","  DJ_Db = []\n","  DJ_DW = []\n","  l = L\n","  while l > 0:\n","    DJ_Db_l, DJ_DW_l, DJ_DA_l_1 = gradients_one_layer(l,layer_type[l],activation[l],paddings[l],strides[l],b[l],W[l],Z[l],A[l],A[l-1],DJ_DA[0],Y,first_dense):\n","    if l == first_dense:\n","      DJ_DA_l_1 = DJ_DA_l_1.reshape(A[l-1].shape)    \n","    DJ_DA.insert(0,DJ_DA_l_1)\n","    DJ_Db.insert(0,DJ_Db_l)\n","    DJ_DW.insert(0,DJ_DW_l)\n","    l = l-1\n","  DJ_Db.insert(0,None)\n","  DJ_DW.insert(0,None)  \n","  return DJ_Db, DJ_DW  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JfI2fbOSaiY1"},"source":["Initializing the parameters"]},{"cell_type":"code","metadata":{"id":"JJyf-nHrak10"},"source":["def initialize_a_W_and_b(layer_type,shape_input,shape_output,filter_size,need_to_flatten):\n","  if need_to_flatten:\n","    s_i_f = shape_input[0]*shape_input[1]*shape_input[2]\n","    W = np.random.randn(s_i_f,shape_output)/np.sqrt(s_i_f)\n","    b = np.zeros(shape_output)\n","    return W, b \n","  if layer_type == 'dense_layer':\n","    W = np.random.randn(shape_input,shape_output)/np.sqrt(shape_input)\n","    b = np.zeros(shape_output)\n","    return W, b\n","  if layer_type == 'convolutional_layer':\n","    s_i_f = shape_input[0]*shape_input[1]*shape_input[2]\n","    W = np.random.randn(filter_size,filter_size,shape_input[2],shape_output[2])/np.sqrt(s_i_f)\n","    b = np.zeros(shape_output[2])\n","    return W, b\n","  return None, None  \n","\n","def initialize_all_W_and_b(layer_type,shape_data,L,filters_sizes,first_dense):\n","  W = [None]\n","  b = [None,]\n","  for l in range(1,L+1):\n","    need_to_flatten = (l == first_dense)\n","    W_l, b_l = initialize_a_W_and_b(layer_type[l],shape_data[l-1],shape_data[l],filters_sizes[l],need_to_flatten)\n","    W.append(W_l)\n","    b.append(b_l)\n","  return W, b "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-pHEL_m9fyxF"},"source":["One step of steepest descent"]},{"cell_type":"code","metadata":{"id":"3gMML6FXbd6o"},"source":["def one_step(W,b,activation,layer_type,filters_sizes,strides,paddings,first_dense,L,X,Y,c,la):\n","  n_e = X.shape[0]\n","  A, Z = data_in_layers(W,b,activation,layer_type,filters_sizes,strides,paddings,first_dense,L,X)\n","  error = error_cross_entropy(Y,A[-1])\n","  DJ_Db, DJ_DW = gradients_all_layers(W,b,activation,layer_type,strides,paddings,first_dense,L,A,Z,Y)\n","  for l in range(1,L+1):\n","    if layer_type[l] != 'pooling_layer':\n","      b[l] = b[l] - c*DJ_Db[l] - 2*la[l]*b[l]/n_e \n","      W[l] = W[l] - c*DJ_DW[l] - 2*la[l]*W[l]/n_e\n","  return b, W, error  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aN1iE4FjgOlv"},"source":["Steepest descent"]},{"cell_type":"code","metadata":{"id":"oNHFJ_o6gQud"},"source":["def all_steps(shape_data,activation,layer_type,filters_sizes,strides,paddings,first_dense,L,X,Y,c,n_steps,la):\n","  W, b = initialize_all_W_and_b(layer_type,shape_data,L,filters_sizes,first_dense)\n","  J = np.array([])\n","  for i in range(n_steps):\n","    b, W, error = one_step(W,b,activation,layer_type,filters_sizes,strides,paddings,first_dense,L,X,Y,c,la)\n","    J = np.append(J,error)\n","  return b, W, J"],"execution_count":null,"outputs":[]}]}